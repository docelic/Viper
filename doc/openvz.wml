
<h2>Viper in OpenVZ CT</h2>
<p>
This is a practical guide on setting up OpenVZ and creating a container
(CT) in which you will install Viper.
</p>
<p>
Installing Viper under CT can be very useful. For example, I have
Lenovo's S10e ideapad that I carry around, and its host node
is set up as regular desktop machine.
</p>
<p>
However, I want to be able to use it as the Viper install server as
well, without introducing Viper-specific configs to the desktop
system. 
</p>
<p>
Welcome to OpenVZ.
</p>

<h3>Host setup</h3>
<p>
Let's first install vz-enabled kernel, after which you should reboot into
it (make sure you select openvz in the bootloader menu, sometimes it
does not become the default kernel).
</p>
<pre>
apt-get install linux-image-openvz-686
</pre>
<p>
After vz support is installed, we can continue. First, define and load the
necessary sysctl parameters:
</p>
<pre>
sudo sh -c 'echo "net.ipv4.ip_forward=1" &gt;&gt; /etc/sysctl.conf'
sudo sh -c 'echo "net.ipv4.conf.eth0.proxy_arp=1" &gt;&gt; /etc/sysctl.conf'

sudo sysctl -p
</pre>

<h3>vzctl</h3>
<p>
We need to use vzctl version &gt;= 3.0.23. It is currently in Debian testing
or unstable. If dpkg -l vzctl shows a lower version, adjust your sources.list
to also contain testing, do 'apt-get install vzctl' to upgrade it, and then
remove testing from sources.list.
</p>
<pre>
1) Add testing to sources.list
2) apt-get update
3) apt-get install vzctl
4) Remove testing from sources.list
5) apt-get update
</pre>

<h3>Installing the container</h3>
<p>
Now let's debootstrap the Debian installation for container 110, apply
necessary vz configuration and add a SSH key so later we can ssh in
without a password:
</p>
<pre>
debootstrap --arch i386 lenny /var/lib/vz/private/110 \\
  http://ftp.us.debian.org/debian

cd /etc/vz/conf

wget http://git.openvz.org/?p=vzctl;a=blob_plain;f=etc/conf/ve-unlimited.conf-sample \\
  -O ve-unlimited.conf-sample

vzctl set 110 --applyconfig unlimited --save
vzctl set 110 --diskspace unlimited:unlimited --save
vzctl set 110 --diskinodes unlimited:unlimited --save
echo "OSTEMPLATE=debian-4.0" &gt;&gt; /etc/vz/conf/110.conf

ln -sf /var/lib/vz/private/110 /viper

sed -i -e '/getty/d' /viper/etc/inittab
echo viper &gt; /viper/etc/hostname

test -r ~/.ssh/id_dsa.pub || ssh-keygen -t dsa
mkdir -p /viper/root/.ssh
cp ~/.ssh/id_dsa.pub /viper/root/.ssh/authorized_keys
</pre>
<p>
Now, to be able to run the DHCP server and have a real ethX interface
within the VZ, we need to use 'veth' instead of the default 'venet'
interface in openvz.
</p>
<p>
Veth devices are added using vzctl --netif_add, and the following
will create devices eth0 and eth1 within the CT. (On the host node,
you'll see veth110.0 and veth110.1).
</p>
<pre>
apt-get install bridge-utils

vzctl set 110 --netif_add eth0,,,,vzbr0 --save
vzctl set 110 --netif_add eth1,,,,vzbr1 --save
</pre>
<p>
(If you get "Invalid syntax" error on the above, as we said, you need
to upgrade your vzctl to at least 3.0.23).
</p>
<p>
Ok. Note that ethX interfaces in the CT are standalone devices that
are not linked to host's ethX. To do that, we'll use bridging.
</p>
<p>
Simply, bridge is a device to which you just add the devices you
want to bridge. 
In our setup, the host's eth0 and eth1 interfaces will become part
of bridges vzbr0 and vzbr1. The container's devices (visible as veth110.0
and veth110.1) are present only while the CT is running, so the vznet.conf
script will take care of adding/removing them from the bridges as CT is
started or stopped.
</p>
<p>
As the host's eth0 and eth1 interfaces become part
of bridges vzbr0 and vzbr1, it means you will remove any eth0 and eth1
configuration in /etc/network/interfaces, add the following bridge
configuration, and remember that from now on, the bridges take over
eth0/eth1, so whatever network operation you want to do, do it on
vzbr0/vzbr1 instead of ethX devices, and you'll get the expected
results.
</p>
<p>
Bridge specifications in /etc/network/interfaces (the listing here
assumes you have two interfaces, eth0 and eth1, where eth0 was a
standard DHCP-configured interface, and eth1 was the device you wanted
to use for Viper and DHCP, so they get to be replaced with bridged
equivalents):
</p>
<pre>
auto vzbr0
iface vzbr0 inet dhcp
	pre-up ifconfig eth0 up
	pre-up brctl addbr vzbr0
	pre-up brctl addif vzbr0 eth0
	post-down ifconfig eth0 down
	post-down brctl delif vzbr0 eth0
	post-down brctl delbr vzbr0

auto vzbr1
iface vzbr1 inet static
	address 10.0.1.254
	netmask 255.255.255.0
	pre-up ifconfig eth1 up
	pre-up brctl addbr vzbr1
	pre-up brctl addif vzbr1 eth1
	post-down ifconfig eth1 down
	post-down brctl delif vzbr1 eth1
	post-down brctl delbr vzbr1
</pre>
<p>
Apply the new interfaces configuration like this
(execute this ONLY locally, DO NOT run over SSH
as it'll kill your connection and leave machine in the unconfigured state):
</p>
<pre>
ifconfig eth0 0
ifconfig eth1 0

ifup vzbr0
ifup vzbr1
</pre>
<p>
Now configure vz to run the script that changes the veth devices in the
bridges as containers are started or stopped.
</p>
<pre>
test -r /etc/vz/vznet.conf || echo '#!/bin/bash' &gt; /etc/vz/vznet.conf
echo 'EXTERNAL_SCRIPT="/usr/sbin/vznetaddbr"' &gt;&gt; /etc/vz/vznet.conf
chmod 755 /etc/vz/vznet.conf
</pre>
<p>
Finally, start the CT110:
</p>
<pre>
vzctl start 110
</pre>

